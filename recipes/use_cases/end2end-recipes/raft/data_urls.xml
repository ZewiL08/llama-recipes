<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
<url>
<loc>http://llama.meta.com/</loc>
</url>
<url>
<loc>http://llama.meta.com/use-policy/</loc>
</url>
<url>
<loc>http://llama.meta.com/responsible-use-guide/</loc>
</url>
<url>
<loc>http://llama.meta.com/llama2/</loc>
</url>
<url>
<loc>http://llama.meta.com/llama2/license/</loc>
</url>
<url>
<loc>http://llama.meta.com/llama2/use-policy/</loc>
</url>
<url>
<loc>http://llama.meta.com/license/</loc>
</url>
<url>
<loc>http://llama.meta.com/code-llama/</loc>
</url>
<url>
<loc>http://llama.meta.com/llama3/</loc>
</url>
<url>
<loc>http://llama.meta.com/llama3/license/</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-guard-2</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/model-cards-and-prompt-formats/meta-code-llama-70b</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-guard-1</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/model-cards-and-prompt-formats/meta-code-llama</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-2</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/getting_the_models</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/getting-the-models/hugging-face</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/getting-the-models/kaggle</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/llama-everywhere</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/llama-everywhere/running-meta-llama-on-linux/</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/llama-everywhere/running-meta-llama-on-windows/</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/llama-everywhere/running-meta-llama-on-mac/</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/llama-everywhere/running-meta-llama-in-the-cloud/</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/how-to-guides/fine-tuning</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/how-to-guides/quantization</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/how-to-guides/prompting</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/how-to-guides/validation</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/integration-guides/meta-code-llama</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/integration-guides/langchain</loc>
</url>
<url>
<loc>http://llama.meta.com/docs/integration-guides/llamaindex</loc>
</url>
<url>
<loc>http://raw.githubusercontent.com/meta-llama/llama-recipes/main/README.md</loc>
</url>
<url>
<loc>http://raw.githubusercontent.com/meta-llama/llama/main/MODEL_CARD.md</loc>
</url>
<url>
<loc>http://raw.githubusercontent.com/meta-llama/llama/main/README.md</loc>
</url>
<url>
<loc>http://raw.githubusercontent.com/meta-llama/llama3/main/MODEL_CARD.md</loc>
</url>
<url>
<loc>http://raw.githubusercontent.com/meta-llama/llama3/main/README.md</loc>
</url>
<url>
<loc>http://raw.githubusercontent.com/meta-llama/codellama/main/MODEL_CARD.md</loc>
</url>
<url>
<loc>http://raw.githubusercontent.com/meta-llama/codellama/main/README.md</loc>
</url>
<url>
<loc>http://raw.githubusercontent.com/meta-llama/PurpleLlama/main/README.md</loc>
</url>
<url>
<loc>http://raw.githubusercontent.com/meta-llama/PurpleLlama/main/Llama-Guard2/MODEL_CARD.md</loc>
</url>
<url>
<loc>http://raw.githubusercontent.com/meta-llama/PurpleLlama/main/Llama-Guard2/README.md</loc>
</url>
<url>
<loc>http://raw.githubusercontent.com/meta-llama/PurpleLlama/main/Llama-Guard/MODEL_CARD.md</loc>
</url>
<url>
<loc>https://hamel.dev/notes/llm/inference/03_inference.html</loc>
</url>
<url>
<loc>https://www.anyscale.com/blog/continuous-batching-llm-inference</loc>
</url>
<url>
<loc>https://github.com/huggingface/peft</loc>
</url><url>
<loc>https://github.com/facebookresearch/llama-recipes/blob/main/docs/LLM_finetuning.md</loc>
</url>
<url>
<loc>https://github.com/meta-llama/llama-recipes/blob/main/recipes/finetuning/datasets/README.md</loc>
</url><url>
<loc>https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms</loc>
</url>
<url>
<loc>https://www.wandb.courses/courses/training-fine-tuning-LLMs</loc>
</url>
<url>
<loc>https://www.snowflake.com/blog/meta-code-llama-testing/</loc>
</url><url>
<loc>https://www.phind.com/blog/code-llama-beats-gpt4</loc>
</url>
<loc>https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper</loc>
</url>
<url>
<loc>https://ragntune.com/blog/gpt3.5-vs-llama2-finetuning</loc>
</url><url>
<loc>https://deci.ai/blog/fine-tune-llama-2-with-lora-for-question-answering/</loc>
</url>
<url>
<loc>https://replicate.com/blog/fine-tune-translation-model-axolotl</loc>
</url><url>
<loc>https://huyenchip.com/2023/04/11/llm-engineering.html</loc>
</url>
</urlset>
